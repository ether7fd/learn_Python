\documentclass[uplatex,titlepage]{jsarticle}
%\documentclass[draft]{jsarticle}
\usepackage[top=20truemm,bottom=20truemm,left=20truemm,right=20truemm]{geometry}
\usepackage{listings}
\usepackage[dvipdfmx]{graphicx}
%\usepackage[draft]{graphicx}
\usepackage{indentfirst}
\usepackage{amsmath,amssymb}
\usepackage{here}
\usepackage{url}
\usepackage{fancybox}
\usepackage{multirow}
\usepackage{ascmac}
\usepackage{array}
\graphicspath{{./img/}}

\newif\iffigure
% \figurefalse
\figuretrue

\lstset{%
  language={Python},
  basicstyle={\small},%
  identifierstyle={\small},%
  commentstyle={\small\itshape},%
  keywordstyle={\small\bfseries},%
  ndkeywordstyle={\small},%
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},%
  numbers=left,%
  xrightmargin=0zw,%
  xleftmargin=3zw,%
  numberstyle={\scriptsize},%
  stepnumber=1,
  numbersep=1zw,%
  lineskip=-0.5ex%
}

\title{レポート作成技術}
\author{電気電子システム工学科5年　貝沼遼太郎}
\date{2021年05月11日}

\begin{document}
\maketitle
\section{目的}
機械学習手法を用いた画像の2値分類を目標とし，データの準備からモデルの構築，学習，改善の一連の流れについて学ぶ．また機械学習において重要なニューラルネットワークのしくみを理解し，畳み込みやプーリングがどのような作用をしているのかを知る．さらに過学習やファインチューニングについて知り，分類精度を向上させる手法について考察する．

\section{課題}
\subsection{ニューラルネットワークの仕組み，モデルの構築から学習，推論の流れについてまとめよ}
\subsubsection{今回の2値分類で用いた学習モデル}
今回の分類では，主に畳み込み層とプーリング層を用いて構成したモデルを使用した．このモデルを元に層を追加したり，データを加工したりして学習の改善を図った．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{IMG_3641.JPG} 
    \caption{畳み込み層とプーリング層により構成されたモデル}
    \end{center}
\end{figure}
\fi

\subsubsection{ニューラルネットワークモデル（NeuralNetwork）}
まずは，図のような単純な全結合層を3つほど使った多層ニューラルネットワークモデルを見てみる．\par
いちばん上の行を入力層という．今回のモデルは8個の入力ユニット（ノードともいう）を持っている．\par
上から2番目の層を中間層という．今回のモデルでは6個のユニットを持っている．\par
最後の層を出力層という．今回のモデルでは8個の出力のユニットを持っている．\par
中間層のことを隠れ層と呼ぶ場合もある．また，3層以上のモデルをディープニューラルネットワーク（DeepNeuralNetwork:DNN）と呼ぶ．ユニットとユニットを繋ぐ線の事を，エッジという．各エッジは重みを持つ．つまり，エッジを通るとエッジの重み分の掛け算が実行される．ユニットでは，前の層の各ノードの値がエッジの重みの値をかけたものを総和し，ユニットの値とする．ユニットの値に活性化関数を通したものがそのユニットの値となり次の層に送られる．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{deep_learning_model.png} 
    \caption{多層ニューラルネットワークモデル}
    \end{center}
\end{figure}
\fi
基本は掛けて足す（積和）の繰り返し．\par
行列・ベクトルを使って表記すると$v=\sigma(Wu+b)$\par
Wは重み行列（WeightMatrix）で，ニューロとニューロを繋ぐ線を通ると，Wjnの係数が掛け算される．

\subsubsection{活性化関数（ActivationFunction）}
$\sigma()$は活性化関数（ActivationFunction）でカッコの中の値をReLU関数やtanh関数やシグモイド関数として出力する．$\sigma()$のカッコの中の値をそのまま出力するもの，$\sigma(x)=x$を恒等関数という．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{actication_function.png} 
    \caption{活性化関数}
    \end{center}
\end{figure}
\fi

\subsubsection{畳み込み（Convolution）}
畳み込みは，ニューラルネットワークの入力層に入力する前の前処理として使う．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{convolution_model.png} 
    \caption{畳み込みのイメージ}
    \end{center}
\end{figure}
\fi
画像処理の中核になっているのが畳み込み層．\par
3×3や5×5といったサイズのカーネル（画像処理でいうフィルタ）をスライドさせながら，カーネルの重みを対応する部分行列に掛けて和を取り，バイアスを加えたものを出力の1要素とする．\par
そのままだと出力のサイズが小さくなるがパディングを行ってサイズを変えない場合もある．\par
カラー画像などの場合には，カーネルは3次元になる．RGBなので赤色を表すのに数値として0～255の範囲の値をもち，同様に緑色0～255，青色0～255の3色があるので3次元となる．さらに，PNG等は透明色のαチャンネルをもっており，RGBAの4次元になっている場合もある．\par
カーネルの数が出力のチャネル数になる．

\subsubsection{プーリング（Pooling）}
プーリングも，ニューラルネットワークの入力層に入力する前の前処理として使う．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{pooling_model.png} 
    \caption{プーリングのイメージ}
    \end{center}
\end{figure}
\fi
特徴を抽出しサイズを縮小するのがプーリング層の役割．\par
プールサイズは画像処理では2×2がよく使われる．\par
2×2のサイズで2個ずつスライドさせれば画像は縦横それぞれ半分のサイズになる．\par
出力のチャネル数は変わらない．\par
画像処理では最大値を取るMaxプーリングをよく使う．

\subsubsection{畳み込みニューラルネットワーク（ConvolutionDeepNeuralNetwork）}
CNNでは，下図のように隠れ層は「畳み込み層」と「プーリング層」で構成される．\par
畳み込み層は，前の層で近くにあるノードにフィルタ処理して「特徴マップ」を得る．\par
プーリング層は，畳み込み層から出力された特徴マップを，さらに縮小して新たな特徴マップとする．\par
この際に着目する領域のどの値を用いるかだが，図のように最大値を得ることで，画像の多少のずれも吸収される．\par
したがって，この処理により画像の位置移動に対する不偏性を獲得したことになる．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{CNN_model.png} 
    \caption{畳み込みニューラルネットワーク}
    \end{center}
\end{figure}
\fi
畳み込み層は画像の局所的な特徴を抽出し，プーリング層は局所的な特徴をまとめあげる処理をしている．\par
つまり，これらの処理の意味するところは，入力画像の特徴を維持しながら画像を縮小処理していることになる．\par
今までの画像縮小処理と異なるところは，画像の特徴を維持しながら画像の持つ情報量を大幅に圧縮できるところである．これを言い換えると，画像の「抽象化」ともいう．

\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{CNN_model2.png} 
    \caption{畳み込みニューラルネットワーク2}
    \end{center}
\end{figure}
\fi

\subsection{画像分類の精度を上げるため，keras\_hands-on\_textbook.ipynbのコードを参考にして精度向上を行え．精度向上前と精度向上後とを比較し，改善あるいは劣化を考察せよ}


\subsection{過学習とはどのようなことを言うのか，また今回の実験で過学習が発生しているかを考察せよ}
過学習とは，機械学習のモデルが訓練用データに最適化しすぎて，新しデータに対応できなくなることをいう．いわばモデルが訓練用データを丸暗記した状態である．つまり少しの応用問題，イレギュラーデータに対応できない状態である．これはモデルのパラメータ数に対して，訓練用データが少なすぎる場合に発生するとされている．\par
下図は過学習が発生したと思われる学習のモデルである．この時，学習に用いたtrainingデータは110ほどであったのに対して，モデルのパラメータ数は3,321,410となっている．よってこの学習ではモデルのパラメータ数に対して学習データが少なかったために過学習が発生したと考えられる．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{overfit_model.png} 
    \caption{過学習が発生した時のモデル}
    \end{center}
\end{figure}
\fi

下図は過学習が発生したときの学習推移のグラフである．trainingデータでの学習は正答率が増加し，損失が減少していることが分かるが，validationデータでの検証は正答率が減少し，損失が増加していることが分かる．よって，この学習はtrainingデータに最適化され，イレギュラーなデータに対応できなくなっている事が分かる．
\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{ker_CNN_acc.png} \\
   \caption{過学習と考えられる正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{ker_CNN_loss.png} \\
    \caption{過学習と考えられる損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi


\subsection{過学習を防止するための手法について考察せよ}
前節のような過学習を抑制するための方法として，ドロップアウト層の導入，バッチ正規化，パラメータ正則化の3つが挙げられる．以下にそれぞれの手法を用いたときの学習結果を示す．
\subsubsection{ドロップアウトの導入}
ドロップアウトとは，レイヤー（層）の出力をランダムにゼロとすることで，過学習を抑制する手法のことである．副作用として，訓練の収束するまでの時間が長くなるデメリットがある．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{dropout_model.png} 
    \caption{ドロップアウト層を追加したモデル}
    \end{center}
\end{figure}
\fi
下図の学習の推移のグラフより，ドロップアウト層を用いることで正答率，損失が改善し，過学習が抑えられていることが分かる．しかし，学習の終盤（20epoch～）においてもグラフにまだ傾きがあることから，学習が足りてないのでは無いかと考えられる．よって，ドロップアウト層を導入すると学習の精度は向上するものの収束に時間がかかり，学習時間が長くなることが分かった．
\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{dropout_acc.png} \\
   \caption{ドロップアウトの導入による正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{dropout_loss.png} \\
    \caption{ドロップアウトの導入による損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi


\subsubsection{バッチ正規化}
バッチ正規化とは，層の出力をバッチ単位で正規化（平均0と分散1）とすることである．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{regular_model.png} 
    \caption{バッチ正規化のモデル}
    \end{center}
\end{figure}
\fi
下図は，バッチ正規化を適用したときの学習の推移グラフである．trainデータでは正答率，損失がともに良好に収束している事が分かる．しかし，Validationデータではtrainデータの学習に追随していないため，過学習が起きていると考えられる．よって，今回の学習ではバッチ正規化が過学習に有効であるかは分からなかった．またグラフから，trainデータは少ないepoch数（4epoch～）からほとんど収束している事が分かる．よって，バッチ正規化には学習を早くしたり効率を上げるメリットがあるのではないかと考えられる．
\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{regular_acc.png} \\
   \caption{バッチ正規化したときの正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{regular_loss.png} \\
    \caption{バッチ正規化したときの損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi


\subsubsection{パラメータ正則化}
損失関数にパラメータの「大きさ」に連動してた「ペナルティ」を加えることでパラメータのばらつきを抑制する手法である．これによって近似関数が複雑な形（オーバーフィッテイング）になる事を防ぐ役割がある．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{param_regular_model.png} 
    \caption{パラメータ正則化のモデル}
    \end{center}
\end{figure}
\fi
下図は，パラメータ正則化を適用したときの学習の推移グラフである．損失の推移がきれいに右肩下がりに減少し，収束しているのに対して正答率は0.50あたりをほぼ横ばいで推移している．正答率に関しては分からないが，パラメータ正則化は損失を抑えるのに長けている手法であると考えられる．
\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{param_regular_acc.png} \\
   \caption{パラメータ正則化したときの正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{param_regular_loss.png} \\
    \caption{パラメータ正則化したときの損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi

\subsection{ファインチューニングの手法について考察せよ}
ファインチューニングとは，学習済みネットワークを用いた学習のことである．学習済みネットワークとはあるデータセットで訓練を行った後の，モデルアーキテクチャとパラメータ重みのセットの事である．また学習済みネットワークの重みを固定し，新たなネットワークに組み込んで訓練を行うことを転移学習という．ファインチューニングは，転移学習の後，学習済みネットワークの出力に近い部分のみを目的に合わせて微調整することをいう．\par
下の図はVGG16の内部層を示したもので，その下の図は今回の実装においてVGG16を利用したモデルを示したものである．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{vgg16_model.png} 
    \caption{VGG16モデル}
    \end{center}
\end{figure}
\fi
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{finetyu_before.png} 
    \caption{ファインチューニングに用いたモデル}
    \end{center}
\end{figure}
\fi
モデルの重みを固定して訓練した様子を下のグラフに示す．いわば，VGG16の学習済みモデルをそのままラーメンとうどんの分類に適用しただけの学習モデルである．\par
グラフを見ると正答率はTrain,Valiationともに増加し，損失は減少していることが分かる．また，Validationの時の方が良い結果（val\_acc>traing\_acc, val\_loss<train\_loss）が得られていることから，学習は十分であったと考えられる．
\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{finetyu_acc.png} \\
   \caption{重みを凍結して学習したときの正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{finetyu_loss.png} \\
    \caption{重みを凍結して学習したときの損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi

次に，ファインチューニングを行うために凍結したconv\_layersのblock5の層の凍結を解除した．Trainable paramsの値が増えていることが分かる．\par
そして，学習させた結果が下の図である．同じように正答率が右肩上がりに増加し，損失はきれいに減少していることが分かる．ここで注目したいのがエポック数である．この学習では重みを凍結して学習したときの半分のエポック数に設定してある．にも関わらず，最終的な正答率，損失はそのそれを上回っている事が分かる．この事から，ファインチューニングはもともと膨大な数の学習がされたモデルを用いているため，少ない量の学習で済むと考えられる．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{finetyu_after.png} 
    \caption{凍結解除後のモデル}
    \end{center}
\end{figure}
\fi

\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{finetyu_after_acc.png} \\
   \caption{凍結解除して学習したときの正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{finetyu_after_loss.png} \\
    \caption{凍結解除して学習したときの損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi



\section{参考文献}
\begin{itemize}
  \item 「Pythonによるデータ解析応用編」-矢野 昌平
\end{itemize}

\end{document}