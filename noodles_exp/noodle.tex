\documentclass[uplatex,titlepage]{jsarticle}
%\documentclass[draft]{jsarticle}
\usepackage[top=20truemm,bottom=20truemm,left=20truemm,right=20truemm]{geometry}
\usepackage{listings}
\usepackage[dvipdfmx]{graphicx}
%\usepackage[draft]{graphicx}
\usepackage{indentfirst}
\usepackage{amsmath,amssymb}
\usepackage{here}
\usepackage{url}
\usepackage{fancybox}
\usepackage{multirow}
\usepackage{ascmac}
\usepackage{array}
\graphicspath{{./img/}}

\newif\iffigure
% \figurefalse
\figuretrue

\lstset{%
  language={Python},
  basicstyle={\small},%
  identifierstyle={\small},%
  commentstyle={\small\itshape},%
  keywordstyle={\small\bfseries},%
  ndkeywordstyle={\small},%
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},%
  numbers=left,%
  xrightmargin=0zw,%
  xleftmargin=3zw,%
  numberstyle={\scriptsize},%
  stepnumber=1,
  numbersep=1zw,%
  lineskip=-0.5ex%
}

\title{レポート作成技術}
\author{電気電子システム工学科5年　貝沼遼太郎}
\date{2021年05月11日}

\begin{document}
\maketitle
\section{目的}

\section{課題}
\subsection{ニューラルネットワークの仕組み，モデルの構築から学習，推論の流れについてまとめよ}
\subsubsection{ニューラルネットワークモデル（NeuralNetwork）}
まずは，図のような単純な全結合層を3つほど使った多層ニューラルネットワークモデルを見てみる．\par
いちばん上の行を入力層という．今回のモデルは8個の入力ユニット（ノードともいう）を持っている．\par
上から2番目の層を中間層という．今回のモデルでは6個のユニットを持っている．\par
最後の層を出力層という．今回のモデルでは8個の出力のユニットを持っている．\par
中間層のことを隠れ層と呼ぶ場合もある．また，3層以上のモデルをディープニューラルネットワーク（DeepNeuralNetwork:DNN）と呼ぶ．ユニットとユニットを繋ぐ線の事を，エッジという．各エッジは重みを持つ．つまり，エッジを通るとエッジの重み分の掛け算が実行される．ユニットでは，前の層の各ノードの値がエッジの重みの値をかけたものを総和し，ユニットの値とする．ユニットの値に活性化関数を通したものがそのユニットの値となり次の層に送られる．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{deep_learning_model.png} 
    \caption{多層ニューラルネットワークモデル}
    \end{center}
\end{figure}
\fi
基本は掛けて足す（積和）の繰り返し．\par
行列・ベクトルを使って表記すると$v=\sigma(Wu+b)$\par
Wは重み行列（WeightMatrix）で，ニューロとニューロを繋ぐ線を通ると，Wjnの係数が掛け算される．

\subsubsection{活性化関数（ActivationFunction）}
$\sigma()$は活性化関数（ActivationFunction）でカッコの中の値をReLU関数やtanh関数やシグモイド関数として出力する．$\sigma()$のカッコの中の値をそのまま出力するもの，$\sigma(x)=x$を恒等関数という．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{actication_function.png} 
    \caption{活性化関数}
    \end{center}
\end{figure}
\fi

\subsubsection{畳み込み（Convolution）}
畳み込みは，ニューラルネットワークの入力層に入力する前の前処理として使う．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{convolution_model.png} 
    \caption{畳み込みのイメージ}
    \end{center}
\end{figure}
\fi
画像処理の中核になっているのが畳み込み層．\par
3×3や5×5といったサイズのカーネル（画像処理でいうフィルタ）をスライドさせながら，カーネルの重みを対応する部分行列に掛けて和を取り，バイアスを加えたものを出力の1要素とする．\par
そのままだと出力のサイズが小さくなるがパディングを行ってサイズを変えない場合もある．\par
カラー画像などの場合には，カーネルは3次元になる．RGBなので赤色を表すのに数値として0～255の範囲の値をもち，同様に緑色0～255，青色0～255の3色があるので3次元となる．さらに，PNG等は透明色のαチャンネルをもっており，RGBAの4次元になっている場合もある．\par
カーネルの数が出力のチャネル数になる．

\subsubsection{プーリング（Pooling）}
プーリングも，ニューラルネットワークの入力層に入力する前の前処理として使う．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{pooling_model.png} 
    \caption{プーリングのイメージ}
    \end{center}
\end{figure}
\fi
特徴を抽出しサイズを縮小するのがプーリング層の役割．\par
プールサイズは画像処理では2×2がよく使われる．\par
2×2のサイズで2個ずつスライドさせれば画像は縦横それぞれ半分のサイズになる．\par
出力のチャネル数は変わらない．\par
画像処理では最大値を取るMaxプーリングをよく使う．

\subsubsection{畳み込みニューラルネットワーク（ConvolutionDeepNeuralNetwork）}
CNNでは，下図のように隠れ層は「畳み込み層」と「プーリング層」で構成される．\par
畳み込み層は，前の層で近くにあるノードにフィルタ処理して「特徴マップ」を得る．\par
プーリング層は，畳み込み層から出力された特徴マップを，さらに縮小して新たな特徴マップとする．\par
この際に着目する領域のどの値を用いるかだが，図のように最大値を得ることで，画像の多少のずれも吸収される．\par
したがって，この処理により画像の位置移動に対する不偏性を獲得したことになる．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{CNN_model.png} 
    \caption{畳み込みニューラルネットワーク}
    \end{center}
\end{figure}
\fi
畳み込み層は画像の局所的な特徴を抽出し，プーリング層は局所的な特徴をまとめあげる処理をしている．\par
つまり，これらの処理の意味するところは，入力画像の特徴を維持しながら画像を縮小処理していることになる．\par
今までの画像縮小処理と異なるところは，画像の特徴を維持しながら画像の持つ情報量を大幅に圧縮できるところである．これを言い換えると，画像の「抽象化」ともいう．

\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{CNN_model2.png} 
    \caption{畳み込みニューラルネットワーク2}
    \end{center}
\end{figure}
\fi

\subsection{画像分類の精度を上げるため，keras\_hands-on\_textbook.ipynbのコードを参考にして精度向上を行え．精度向上前と精度向上後とを比較し，改善あるいは劣化を考察せよ}


\subsection{過学習とはどのようなことを言うのか，また今回の実験で過学習が発生しているかを考察せよ}
過学習とは，機械学習のモデルが訓練用データに最適化しすぎて，新しデータに対応できなくなることをいう．いわばモデルが訓練用データを丸暗記した状態である．つまり少しの応用問題，イレギュラーデータに対応できない状態である．これはモデルのパラメータ数に対して，訓練用データが少なすぎる場合に発生するとされている．\par
下図は過学習が発生したと思われる学習のモデルである．この時，学習に用いたtrainingデータは110ほどであったのに対して，モデルのパラメータ数は3,321,410となっている．よってこの学習ではモデルのパラメータ数に対して学習データが少なかったために過学習が発生したと考えられる．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{overfit_model.png} 
    \caption{過学習が発生した時のモデル}
    \end{center}
\end{figure}
\fi

下図は過学習が発生したときの学習推移のグラフである．trainingデータでの学習は正答率が増加し，損失が減少していることが分かるが，validationデータでの検証は正答率が減少し，損失が増加していることが分かる．よって，この学習はtrainingデータに最適化され，イレギュラーなデータに対応できなくなっている事が分かる．
\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{ker_CNN_acc.png} \\
   \caption{過学習と考えられる正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{ker_CNN_loss.png} \\
    \caption{過学習と考えられる損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi


\subsection{過学習を防止するための手法について考察せよ}
前節のような過学習を抑制するための方法として，ドロップアウト層の導入，バッチ正規化，パラメータ正則化の3つが挙げられる．
\subsubsection{ドロップアウトの導入}
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{dropout_model.png} 
    \caption{ドロップアウト層を追加したモデル}
    \end{center}
\end{figure}
\fi

\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{dropout_acc.png} \\
   \caption{ドロップアウトの導入による正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{dropout_loss.png} \\
    \caption{ドロップアウトの導入による損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi


\subsubsection{バッチ正規化}
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{regular_model.png} 
    \caption{バッチ正規化のモデル}
    \end{center}
\end{figure}
\fi

\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{regular_acc.png} \\
   \caption{バッチ正規化したときの正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{regular_loss.png} \\
    \caption{バッチ正規化したときの損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi


\subsubsection{パラメータ正則化}
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{param_regular_model.png} 
    \caption{パラメータ正則化のモデル}
    \end{center}
\end{figure}
\fi

\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{param_regular_acc.png} \\
   \caption{パラメータ正則化したときの正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{param_regular_loss.png} \\
    \caption{パラメータ正則化したときの損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi

\subsection{ファインチューニングの手法について考察せよ}
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{vgg16_model.png} 
    \caption{VGG16モデル}
    \end{center}
\end{figure}
\fi
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{finetyu_before.png} 
    \caption{ファインチューニングに用いたモデル}
    \end{center}
\end{figure}
\fi

\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{finetyu_acc.png} \\
   \caption{重みを凍結して学習したときの正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{finetyu_loss.png} \\
    \caption{重みを凍結して学習したときの損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi

ファインチューニングを行うために凍結したconv\_layersのblock5の層の凍結を解除．Trainable paramsの値が増えていることが分かる．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.5\textwidth]{finetyu_after.png} 
    \caption{ファインチューニングに用いた学習モデル}
    \end{center}
\end{figure}
\fi

\iffigure
\begin{figure}[H]
\begin{minipage}{8cm}% 
  \begin{center}
   \includegraphics[width=1\textwidth]{finetyu_after_acc.png} \\
   \caption{凍結解除して学習したときの正答率の推移}
  \end{center}
\end{minipage}
\hfill
\begin{minipage}{8cm}%
  \begin{center}
    \includegraphics[width=1\textwidth]{finetyu_after_loss.png} \\
    \caption{凍結解除して学習したときの損失の推移}
  \end{center}
\end{minipage}
\end{figure}
\fi



\section{参考文献}
\begin{itemize}
  \item 「Pythonによるデータ解析応用編」-矢野 昌平
\end{itemize}

\end{document}