\documentclass[uplatex,titlepage]{jsarticle}
%\documentclass[draft]{jsarticle}
\usepackage[top=20truemm,bottom=20truemm,left=20truemm,right=20truemm]{geometry}
\usepackage{listings}
\usepackage[dvipdfmx]{graphicx}
%\usepackage[draft]{graphicx}
\usepackage{indentfirst}
\usepackage{amsmath,amssymb}
\usepackage{here}
\usepackage{url}
\usepackage{fancybox}
\usepackage{multirow}
\usepackage{ascmac}
\usepackage{array}
\graphicspath{{./img/}}

\newif\iffigure
\figurefalse
% \figuretrue

\lstset{%
  language={C},
  basicstyle={\small},%
  identifierstyle={\small},%
  commentstyle={\small\itshape},%
  keywordstyle={\small\bfseries},%
  ndkeywordstyle={\small},%
  stringstyle={\small\ttfamily},
  frame={tb},
  breaklines=true,
  columns=[l]{fullflexible},%
  numbers=left,%
  xrightmargin=0zw,%
  xleftmargin=3zw,%
  numberstyle={\scriptsize},%
  stepnumber=1,
  numbersep=1zw,%
  lineskip=-0.5ex%
}

\title{レポート作成技術}
\author{電気電子システム工学科5年　貝沼遼太郎}
\date{2021年05月11日}

\begin{document}
\maketitle
\section{目的}

\section{課題}
\subsection{ニューラルネットワークの仕組み，モデルの構築から学習，推論の流れについてまとめよ}
\subsubsection{ニューラルネットワークモデル（NeuralNetwork）}
まずは，図のような単純な全結合層を3つほど使った多層ニューラルネットワークモデルを見てみる．\par
いちばん上の行を入力層という．今回のモデルは8個の入力ユニット（ノードともいう）を持っている．\par
上から2番目の層を中間層という．今回のモデルでは6個のユニットを持っている．\par
最後の層を出力層という．今回のモデルでは8個の出力のユニットを持っている．\par
中間層のことを隠れ層と呼ぶ場合もある．また，3層以上のモデルをディープニューラルネットワーク（DeepNeuralNetwork:DNN）と呼ぶ．ユニットとユニットを繋ぐ線の事を，エッジという．各エッジは重みを持つ．つまり，エッジを通るとエッジの重み分の掛け算が実行される．ユニットでは，前の層の各ノードの値がエッジの重みの値をかけたものを総和し，ユニットの値とする．ユニットの値に活性化関数を通したものがそのユニットの値となり次の層に送られる．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{deep_learning_model.png} 
    \caption{多層ニューラルネットワークモデル}
    \end{center}
\end{figure}
\fi
基本は掛けて足す（積和）の繰り返し．\par
行列・ベクトルを使って表記すると$v=\sigma(Wu+b)$\par
Wは重み行列（WeightMatrix）で，ニューロとニューロを繋ぐ線を通ると，Wjnの係数が掛け算される．

\subsubsection{活性化関数（ActivationFunction）}
$\sigma()$は活性化関数（ActivationFunction）でカッコの中の値をReLU関数やtanh関数やシグモイド関数として出力する．$\sigma()$のカッコの中の値をそのまま出力するもの，$\sigma(x)=x$を恒等関数という．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{actication_function.png} 
    \caption{活性化関数}
    \end{center}
\end{figure}
\fi

\subsubsection{畳み込み（Convolution）}
畳み込みは，ニューラルネットワークの入力層に入力する前の前処理として使う．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{convolution_model.png} 
    \caption{畳み込みのイメージ}
    \end{center}
\end{figure}
\fi
画像処理の中核になっているのが畳み込み層．\par
3×3や5×5といったサイズのカーネル（画像処理でいうフィルタ）をスライドさせながら，カーネルの重みを対応する部分行列に掛けて和を取り，バイアスを加えたものを出力の1要素とする．\par
そのままだと出力のサイズが小さくなるがパディングを行ってサイズを変えない場合もある．\par
カラー画像などの場合には，カーネルは3次元になる．RGBなので赤色を表すのに数値として0～255の範囲の値をもち，同様に緑色0～255，青色0～255の3色があるので3次元となる．さらに，PNG等は透明色のαチャンネルをもっており，RGBAの4次元になっている場合もある．\par
カーネルの数が出力のチャネル数になる．

\subsubsection{プーリング（Pooling）}
プーリングも，ニューラルネットワークの入力層に入力する前の前処理として使う．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{pooling_model.png} 
    \caption{プーリングのイメージ}
    \end{center}
\end{figure}
\fi
特徴を抽出しサイズを縮小するのがプーリング層の役割．\par
プールサイズは画像処理では2×2がよく使われる．\par
2×2のサイズで2個ずつスライドさせれば画像は縦横それぞれ半分のサイズになる．\par
出力のチャネル数は変わらない．\par
画像処理では最大値を取るMaxプーリングをよく使う．

\subsubsection{畳み込みニューラルネットワーク（ConvolutionDeepNeuralNetwork）}
CNNでは，下図のように隠れ層は「畳み込み層」と「プーリング層」で構成される．\par
畳み込み層は，前の層で近くにあるノードにフィルタ処理して「特徴マップ」を得る．\par
プーリング層は，畳み込み層から出力された特徴マップを，さらに縮小して新たな特徴マップとする．\par
この際に着目する領域のどの値を用いるかだが，図のように最大値を得ることで，画像の多少のずれも吸収される．\par
したがって，この処理により画像の位置移動に対する不偏性を獲得したことになる．
\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{CNN_model.png} 
    \caption{畳み込みニューラルネットワーク}
    \end{center}
\end{figure}
\fi
畳み込み層は画像の局所的な特徴を抽出し，プーリング層は局所的な特徴をまとめあげる処理をしている．\par
つまり，これらの処理の意味するところは，入力画像の特徴を維持しながら画像を縮小処理していることになる．\par
今までの画像縮小処理と異なるところは，画像の特徴を維持しながら画像の持つ情報量を大幅に圧縮できるところである．これを言い換えると，画像の「抽象化」ともいう．

\iffigure
\begin{figure}[H]% 
    \begin{center}
    \includegraphics[width=0.8\textwidth]{CNN_model2.png} 
    \caption{畳み込みニューラルネットワーク2}
    \end{center}
\end{figure}
\fi

\subsection{画像分類の精度を上げるため，keras\_hands-on\_textbook.ipynbのコードを参考にして精度向上を行え．精度向上前と精度向上後とを比較し，改善あるいは劣化を考察せよ}


\subsection{過学習とはどのようなことを言うのか，また今回の実験で過学習が発生しているかを考察せよ}


\subsection{過学習を防止するための手法について考察せよ}


\subsection{ファインチューニングの手法について考察せよ}


\section{参考文献}
\begin{itemize}
  \item 「Pythonによるデータ解析応用編」-矢野 昌平
\end{itemize}

\end{document}